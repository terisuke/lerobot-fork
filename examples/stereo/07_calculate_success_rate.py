#!/usr/bin/env python
"""
Evaluation Success Rate Calculator

This script calculates the success rate from lerobot-eval results by reading
the eval_info.json file generated by the official lerobot-eval command.

The official lerobot-eval outputs evaluation results to eval_info.json with this structure:
{
  "overall": {
    "avg_sum_reward": float,
    "avg_max_reward": float,
    "pc_success": float,  # Success rate as percentage (0-100)
    "n_episodes": int
  },
  "task_group_name": { ... }  # Per-task-group metrics
}

See src/lerobot/scripts/lerobot_eval.py:596 and :807-815 for output format.

Usage:
    # Calculate success rate from eval_info.json
    python examples/stereo/07_calculate_success_rate.py \
        --eval-info outputs/eval/so101_stereo/eval_info.json

    # Specify custom output file
    python examples/stereo/07_calculate_success_rate.py \
        --eval-info outputs/eval/so101_stereo/eval_info.json \
        --output results.json
"""

import argparse
import json
from pathlib import Path


def calculate_success_rate(
    eval_info_file: str | Path,
    verbose: bool = True,
) -> dict:
    """
    Calculate success rate from lerobot-eval info JSON file.

    The info file should contain an 'overall' key with aggregated metrics including
    'pc_success' (success rate as percentage 0-100), as generated by lerobot-eval.

    Args:
        eval_info_file: Path to eval_info.json file
        verbose: Print detailed information

    Returns:
        Dictionary containing:
            - n_episodes: Total number of episodes
            - pc_success: Success rate as percentage (0-100)
            - avg_sum_reward: Average sum of rewards
            - avg_max_reward: Average max reward
            - per_task_group: Per-task-group metrics (if available)

    Raises:
        FileNotFoundError: If eval info file does not exist
        KeyError: If required 'overall' field is missing
        ValueError: If file format is invalid
    """
    info_path = Path(eval_info_file)

    if not info_path.exists():
        raise FileNotFoundError(f"Eval info file not found: {eval_info_file}")

    if verbose:
        print("=" * 70)
        print("üìä Evaluation Success Rate Calculator")
        print("=" * 70)
        print(f"Eval info file: {eval_info_file}")
        print("=" * 70)

    # Load evaluation info from JSON
    try:
        with open(info_path, 'r') as f:
            info = json.load(f)
    except json.JSONDecodeError as e:
        raise ValueError(f"Invalid JSON format in eval info file: {e}")

    # Extract overall metrics (required field from lerobot-eval)
    if 'overall' not in info:
        raise KeyError(
            f"Eval info file missing required 'overall' field. "
            f"Available fields: {list(info.keys())}"
        )

    overall = info['overall']

    # Extract key metrics
    n_episodes = overall.get('n_episodes', 0)
    pc_success = overall.get('pc_success', 0.0)  # Already a percentage (0-100)
    avg_sum_reward = overall.get('avg_sum_reward', 0.0)
    avg_max_reward = overall.get('avg_max_reward', 0.0)

    results = {
        "n_episodes": n_episodes,
        "pc_success": pc_success,
        "avg_sum_reward": avg_sum_reward,
        "avg_max_reward": avg_max_reward,
    }

    if verbose:
        print(f"\nüìà Overall Results:")
        print(f"  Total episodes:       {n_episodes}")
        print(f"  Success rate:         {pc_success:.1f}%")
        print(f"  Avg sum reward:       {avg_sum_reward:.3f}")
        print(f"  Avg max reward:       {avg_max_reward:.3f}")

    # Extract per-task-group metrics if available
    per_task_group = {}
    for key, value in info.items():
        if key != 'overall' and isinstance(value, dict):
            if 'pc_success' in value:
                per_task_group[key] = {
                    'pc_success': value.get('pc_success', 0.0),
                    'n_episodes': value.get('n_episodes', 0),
                    'avg_sum_reward': value.get('avg_sum_reward', 0.0),
                    'avg_max_reward': value.get('avg_max_reward', 0.0),
                }

    if per_task_group:
        results['per_task_group'] = per_task_group
        if verbose:
            print(f"\nüìã Per-Task-Group Results:")
            for task_group, metrics in per_task_group.items():
                print(f"  {task_group}:")
                print(f"    Success rate:   {metrics['pc_success']:.1f}%")
                print(f"    Episodes:       {metrics['n_episodes']}")

    if verbose:
        print("\n" + "=" * 70)

        # Performance classification
        if pc_success >= 80:
            print("üéâ Excellent performance!")
        elif pc_success >= 60:
            print("üëç Good performance")
        elif pc_success >= 40:
            print("‚ö†Ô∏è  Moderate performance - consider improvements")
        else:
            print("‚ùå Poor performance - significant improvements needed")

        print("=" * 70)

    return results


def save_results(results: dict, output_path: str | Path) -> None:
    """
    Save evaluation results to JSON file.

    Args:
        results: Results dictionary from calculate_success_rate()
        output_path: Path to output JSON file
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w') as f:
        json.dump(results, f, indent=2)

    print(f"\nüíæ Results saved to: {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Calculate success rate from lerobot-eval info JSON"
    )
    parser.add_argument(
        "--eval-info",
        required=True,
        help="Path to eval_info.json file (e.g., outputs/eval/.../eval_info.json)"
    )
    parser.add_argument(
        "--output",
        default=None,
        help="Path to output JSON file (default: <eval-dir>/success_rate.json)"
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress detailed output"
    )
    args = parser.parse_args()

    try:
        results = calculate_success_rate(
            eval_info_file=args.eval_info,
            verbose=not args.quiet,
        )

        # Save results
        output_path = args.output
        if output_path is None:
            info_path = Path(args.eval_info)
            output_path = info_path.parent / "success_rate.json"

        save_results(results, output_path)

        # Return exit code based on success rate
        # 0 = success (>= 60%), 1 = needs improvement (< 60%)
        return 0 if results["pc_success"] >= 60.0 else 1

    except FileNotFoundError as e:
        print(f"‚ùå Error: {e}")
        print("\nExpected file structure:")
        print("  outputs/eval/<eval_name>/eval_info.json")
        print("\nThis file is automatically generated by lerobot-eval.")
        return 1
    except (KeyError, ValueError) as e:
        print(f"‚ùå Error: {e}")
        print("\nThe eval_info.json file should be generated by lerobot-eval and contain:")
        print("  - 'overall': dict with 'pc_success', 'n_episodes', etc.")
        return 1
    except Exception as e:
        print(f"‚ùå Unexpected error: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
